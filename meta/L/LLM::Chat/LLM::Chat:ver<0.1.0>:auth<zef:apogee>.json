{"api":"","auth":"zef:apogee","authors":["Matt Doughty"],"depends":["Cro::HTTP","Digest::SHA256::Native","JSON::Fast","Tokenizers","UUID::V4"],"description":"Simple framework for LLM inferencing","dist":"LLM::Chat:ver<0.1.0>:auth<zef:apogee>","license":"Artistic-2.0","name":"LLM::Chat","provides":{"LLM::Chat::Backend":"lib/LLM/Chat/Backend.rakumod","LLM::Chat::Backend::KoboldCpp":"lib/LLM/Chat/Backend/KoboldCpp.rakumod","LLM::Chat::Backend::OpenAICommon":"lib/LLM/Chat/Backend/OpenAICommon.rakumod","LLM::Chat::Backend::Response":"lib/LLM/Chat/Backend/Response.rakumod","LLM::Chat::Backend::Response::Stream":"lib/LLM/Chat/Backend/Response/Stream.rakumod","LLM::Chat::Backend::Settings":"lib/LLM/Chat/Backend/Settings.rakumod","LLM::Chat::Conversation":"lib/LLM/Chat/Conversation.rakumod","LLM::Chat::Conversation::Message":"lib/LLM/Chat/Conversation/Message.rakumod","LLM::Chat::Template":"lib/LLM/Chat/Template.rakumod","LLM::Chat::Template::ChatML":"lib/LLM/Chat/Template/ChatML.rakumod","LLM::Chat::Template::Gemma2":"lib/LLM/Chat/Template/Gemma2.rakumod","LLM::Chat::Template::Llama3":"lib/LLM/Chat/Template/Llama3.rakumod","LLM::Chat::Template::Llama4":"lib/LLM/Chat/Template/Llama4.rakumod","LLM::Chat::Template::MistralV7":"lib/LLM/Chat/Template/MistralV7.rakumod","LLM::Chat::TokenCounter":"lib/LLM/Chat/TokenCounter.rakumod"},"release-date":"2025-06-29","source-url":"https://raw.githubusercontent.com/raku/REA/main/archive/L/LLM%3A%3AChat/LLM%3A%3AChat%3Aver%3C0.1.0%3E%3Aauth%3Czef%3Aapogee%3E.tar.gz","support":{"bugtracker":"https://github.com/m-doughty/LLM_Chat/issues","source":"https://github.com/m-doughty/LLM_Chat"},"tags":["llm","inferencing","ai"],"version":"0.1.0"}